{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -O 'https://raw.github.com/sid-unizar/LRC/main/datasets/datasets.zip'\n",
    "!unzip -o datasets.zip\n",
    "!curl -L -O 'https://raw.github.com/sid-unizar/LRC/main/scripts/scripts.zip'\n",
    "!unzip -o scripts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# All the models used in the paper, we used just roberta-base because of time and GPU constraints\n",
    "models = [\"roberta-base\", \"roberta-large\", \"bert-large-uncased-whole-word-masking\", \"bert-base-uncased\"]\n",
    "\n",
    "train_templates = [\n",
    "    \"' <W1> ' <SEP> ' <W2> '\",  # T1\n",
    "     \" <W1> <SEP> <W2> \",  # T2\n",
    "    \"Today, I finally discovered the relation between <W1> and <W2>.\", #T3\n",
    "    \"Today, I finally discovered the relation between <W1> and <W2>: <W1> is the <LABEL> of <W2>.\" #T4\n",
    "]\n",
    "\n",
    "test_templates = [\n",
    "    \"' <W1> ' <SEP> ' <W2> '\",  # T1\n",
    "     \" <W1> <SEP> <W2> \",  # T2\n",
    "    \"Today, I finally discovered the relation between <W1> and <W2>.\", #T3\n",
    "    \"Today, I finally discovered the relation between <W1> and <W2>.\" #T4\n",
    "]\n",
    "\n",
    "datasets = [\"hyperlex/lexical\", \"hyperlex/random\"]\n",
    "\n",
    "# Directory paths\n",
    "script_path = \"/content/scripts/gradedLE_train_evaluate.py\"\n",
    "dataset_base_path = \"/content/datasets/\"\n",
    "output_dir = \"/content/res/\"\n",
    "\n",
    "# Iterate over each dataset, model, and template\n",
    "for dataset in datasets:\n",
    "    train_file = f\"{dataset_base_path}{dataset}/train.tsv\"\n",
    "    test_file = f\"{dataset_base_path}{dataset}/test.tsv\"\n",
    "    val_file = f\"{dataset_base_path}{dataset}/val.tsv\"\n",
    "\n",
    "    for model in models:\n",
    "      for index, template in enumerate(train_templates):\n",
    "          # Construct the command\n",
    "\n",
    "        command = [\n",
    "            \"python\", script_path,\n",
    "            \"--train_templates\", template,\n",
    "            \"--test_templates\", test_templates[index],\n",
    "            \"--model\", \"roberta-base\",\n",
    "            \"--nepochs\", \"10\",\n",
    "            \"--dir_output_results\", output_dir,\n",
    "            \"--batch_size\", \"32\",\n",
    "            \"--warm_up\", \"0.1\",\n",
    "            \"--nrepetitions\", \"1\",\n",
    "            \"--dataset\", \"hyperlex\",\n",
    "            \"--date\", datetime.now().strftime(\"%D-%H:%M:%S\"),\n",
    "            \"--train_file\", train_file,\n",
    "            \"--test_file\", test_file,\n",
    "            \"--val_file\", val_file  # Omit or modify this line if there's no validation dataset\n",
    "        ]\n",
    "\n",
    "          # Execute the command\n",
    "        with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                print(line, end='')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
